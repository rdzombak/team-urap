import numpy as np
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity

import nltk
from nltk.corpus import stopwords

import gensim
from gensim.models import KeyedVectors
from gensim.test.utils import datapath
import GoalCongruence as gc

wv_model = KeyedVectors.load_word2vec_format(r'~/Documents/GitHub/team-urap/test_data/GoogleNews-vectors-negative300.bin.gz', binary=True)

def word2vec_embed(document):
    """Returns list of sentence vectors generated from DOCUMENT, a list of strings."""

    sentence_vectors = []
    for member in document:
        statement = gc.tokenize(member.lower(), ",.?!;:/\|[]{}()<>& _")
        sentence_vector = np.zeros(300)
        for word in statement:
            if word not in stopwords.words('english') and word in wv_model.vocab:
                sentence_vector = np.array(wv_model[word]) + sentence_vector
        if len(statement):
            sentence_vector /= len(statement)
        sentence_vectors.append(list(sentence_vector))
    return sentence_vectors

def word2vec_wmd(series):
    """Returns a list of matrices generated by comparing
       entries within the list of each row in the input SERIES."""

    wv_model.init_sims(replace=True)  # normalizes word2vec vectors
    matrices = []
    for entry in series:
        statement_list = []
        matrix = []
        for statement in entry:
            statement_list.append(gc.tokenize(statement.lower(), ",.?!;:/\|[]{}()<>& _"))
        for statement in statement_list:
            matrix_row = []
            for other_statement in statement_list:
                matrix_row.append(wv_model.wmdistance(statement, other_statement))
            matrix.append(matrix_row)
        matrices.append(matrix)
    return matrices